{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MASArena \ud83c\udfdf\ufe0f !","text":"<p>A comprehensive framework for benchmarking single and multi-agent systems across a wide range of tasks\u2014evaluating performance, accuracy, and efficiency with built-in visualization and tool integration.</p> <p></p>"},{"location":"#core-features","title":"\ud83c\udf1f Core Features","text":"<ul> <li>\ud83e\uddf1 Modular Design: Swap agents, tools, datasets, prompts, and evaluators with ease.</li> <li>\ud83d\udce6 Built-in Benchmarks: Single/multi-agent datasets for direct comparison.</li> <li>\ud83d\udcca Visual Debugging: Inspect interactions, accuracy, and tool use.</li> <li>\ud83d\udd27 Tool Support:  Manage tool selection via pluggable wrappers.</li> <li>\ud83e\udde9 Easy Extensions: Add agents via subclassing\u2014no core changes.</li> <li>\ud83d\udcc2 Paired Datasets &amp; Evaluators: Add new benchmarks with minimal effort.</li> </ul>"},{"location":"#contributing","title":"\ud83d\ude4c Contributing","text":"<p>We warmly welcome contributions from the community!</p> <p>You can contribute in many ways:</p> <ul> <li> <p>\ud83e\udde0 New Agent Systems (MAS):   Add novel single- or multi-agent systems to expand the diversity of strategies and coordination models.</p> </li> <li> <p>\ud83d\udcca New Benchmark Datasets:   Bring in domain-specific or task-specific datasets (e.g., reasoning, planning, tool-use, collaboration) to broaden the scope of evaluation.</p> </li> <li> <p>\ud83d\udee0 New Tools &amp; Toolkits:   Extend the framework's tool ecosystem by integrating domain tools (e.g., search, calculators, code editors) and improving tool selection strategies.</p> </li> <li> <p>\u2699\ufe0f Improvements &amp; Utilities:   Help with performance optimization, failure handling, asynchronous processing, or new visualizations.</p> </li> </ul>"},{"location":"aflow_optimize/","title":"AFlowOptimizer User Guide","text":""},{"location":"aflow_optimize/#introduction","title":"Introduction","text":"<p>AFlowOptimizer is a core component of the MASArena framework for automated optimization of multi-agent workflows. It leverages LLM-driven evolutionary optimization to automatically modify and evaluate workflow code, aiming to improve performance on a specified benchmark.</p> <p>AFlow supports multi-round iterative optimization. In each round, it generates new workflow variants based on historical performance, validates them on evaluation sets, and selects the best-performing solution. The final optimized agent is then evaluated using the standard <code>BenchmarkRunner</code>, ensuring consistent metrics and enabling access to visualization and failure analysis tools.</p>"},{"location":"aflow_optimize/#key-features","title":"Key Features","text":"<ul> <li>Automated Evolutionary Optimization: Uses LLM feedback to automatically modify workflow structure and prompts.</li> <li>Multi-round Iteration: Supports multiple optimization rounds and convergence checks.</li> <li>Integrated Evaluation: Optimized agents are evaluated through the standard <code>BenchmarkRunner</code> for consistent results.</li> <li>Benchmark Agnostic: Works with various benchmarks (e.g., humaneval, math).</li> <li>Highly Extensible: Supports custom operators, agents, and evaluators.</li> </ul>"},{"location":"aflow_optimize/#quick-start","title":"Quick Start","text":""},{"location":"aflow_optimize/#1-environment-setup","title":"1. Environment Setup","text":"<ul> <li>Ensure you have set the following environment variables (e.g., in a <code>.env</code> file):</li> <li><code>OPENAI_API_KEY</code></li> <li><code>OPENAI_API_BASE</code></li> <li>(Optional) <code>OPTIMIZER_MODEL_NAME</code>, <code>EXECUTOR_MODEL_NAME</code></li> </ul>"},{"location":"aflow_optimize/#2-run-optimization-and-evaluation","title":"2. Run Optimization and Evaluation","text":"<p>The optimization process is now integrated into the main benchmark runner. Use the <code>run_benchmark.sh</code> script and specify an optimizer as the last argument.</p> <pre><code># General usage\n./run_benchmark.sh [benchmark] [agent_system] [limit] [mcp_config] [concurrency] [optimizer]\n\n# Example: Run AFlow optimization on humaneval\n./run_benchmark.sh humaneval single_agent 10 \"\" 1 aflow\n</code></pre> <p>This command will: 1.  Run the AFlow optimization process for the <code>humaneval</code> benchmark. 2.  Once optimization is complete, it will automatically run a standard benchmark evaluation on the newly optimized agent. 3.  The results will be saved in the <code>results/</code> directory, compatible with visualization and failure analysis tools.</p>"},{"location":"aflow_optimize/#main-script-arguments","title":"Main Script Arguments","text":"<p>The following arguments in <code>main.py</code> control the optimization process.</p> Argument Type Default Description <code>--run-optimizer</code> str <code>None</code> Specifies the optimizer to run. Use <code>aflow</code>. <code>--benchmark</code> str <code>humaneval</code> Benchmark to optimize for. <code>--graph_path</code> str <code>mas_arena/configs/aflow</code> Path to the base AFlow graph configuration. <code>--optimized_path</code> str <code>example/aflow/humaneval/optimization</code> Path to save the optimized AFlow graph and intermediate files. <code>--validation_rounds</code> int 1 Number of validation rounds per optimization cycle. <code>--eval_rounds</code> int 1 Number of evaluation rounds per optimization cycle. <code>--max_rounds</code> int 3 Maximum number of optimization rounds."},{"location":"aflow_optimize/#example-standalone-usage-advanced","title":"Example Standalone Usage (Advanced)","text":"<p>While the integrated workflow is recommended, you can run the optimization process standalone by executing <code>example/aflow/run_aflow_optimize.py</code>. This will only generate the optimized graph without running the final evaluation.</p> <pre><code># This example is simplified from example/aflow/run_aflow_optimize.py\nimport os\nfrom dotenv import load_dotenv\nfrom mas_arena.agents import AgentSystemRegistry\nfrom mas_arena.evaluators import BENCHMARKS\nfrom mas_arena.optimizers.aflow.aflow_optimizer import AFlowOptimizer\nfrom mas_arena.optimizers.aflow.aflow_experimental_config import EXPERIMENTAL_CONFIG\n\n# --- Configuration ---\nBENCHMARK_NAME = \"humaneval\"\n# ... (load env vars and models) ...\n\n# --- Initialization ---\n# ... (initialize optimizer_agent, executor_agent, evaluator) ...\n\n# --- Optimizer Setup ---\noptimizer = AFlowOptimizer(\n    # ... (optimizer parameters) ...\n)\n\n# --- Run Optimization ---\noptimizer.setup()\noptimizer.optimize(evaluator)\n# The optimized graph is saved in your optimized_path\n\n# To evaluate, you must then run the main benchmark script:\n# python main.py --benchmark humaneval --agent-system single_agent --agent-graph-config path/to/your/final_graph.json\n</code></pre>"},{"location":"aflow_optimize/#integrated-workflow","title":"Integrated Workflow","text":"<ol> <li>Trigger: The user runs <code>main.py</code> with <code>--run-optimizer aflow</code>.</li> <li>Optimization: The <code>AFlowOptimizer</code> is invoked. It iteratively generates and evaluates workflow variants, producing a <code>final_graph.json</code> in the specified <code>optimized_path</code>.</li> <li>Evaluation: <code>main.py</code> automatically takes the path to <code>final_graph.json</code>.</li> <li>Benchmark Run: The <code>BenchmarkRunner</code> is called to execute a standard benchmark on a <code>single_agent</code> configured with the new optimized graph.</li> <li>Results: The results are saved in the standard format, making them available for all downstream analysis and visualization tools.</li> </ol>"},{"location":"aflow_optimize/#faq","title":"FAQ","text":"<p>Q: What models are used for optimization and execution? A: By default, <code>gpt-4o</code> for optimization and <code>gpt-4o-mini</code> for execution. You can override these via the <code>OPTIMIZER_MODEL_NAME</code> and <code>EXECUTOR_MODEL_NAME</code> environment variables.</p> <p>Q: How do I evaluate an optimized agent again later? A: Run the main benchmark script and point to the optimized graph file using the <code>--agent-graph-config</code> argument: <code>python main.py --benchmark humaneval --agent-system single_agent --agent-graph-config path/to/final_graph.json</code></p> <p>Q: Where are the optimized workflows saved? A: In the directory specified by <code>--optimized_path</code>. The final, best-performing graph is saved as <code>final_graph.json</code>.</p>"},{"location":"aflow_optimize/#references","title":"References","text":"<ul> <li>See <code>run_benchmark.sh</code> and <code>main.py</code> for the primary usage pattern.</li> <li>See <code>example/aflow/run_aflow_optimize.py</code> for a reference on running standalone optimization.</li> <li>See <code>mas_arena/optimizers/aflow/aflow_optimizer.py</code> for the core optimizer implementation.</li> <li>See <code>mas_arena/optimizers/aflow/aflow_experimental_config.py</code> for benchmark-specific configurations.</li> </ul>"},{"location":"extending/","title":"\ud83d\ude80 Extending MASArena Framework","text":"<p>A comprehensive guide to extending MASArena with custom Multi-Agent Systems and Evaluators.</p>"},{"location":"extending/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ul> <li>\ud83e\udd16 Multi-Agent System Extension</li> <li>\ud83d\udccb Implementation Requirements</li> <li>\ud83d\udcdd Implementation Steps</li> <li>\u26a1 Advanced Features</li> <li>\ud83d\udca1 Complete Example</li> <li>\ud83c\udfaf Evaluator Extension</li> <li>\ud83d\udd27 Basic Implementation</li> <li>\u26a1 Advanced Features</li> <li>\ud83d\udcbb Code Evaluation</li> <li>\ud83d\udcd6 Complete Examples</li> <li>\u2705 Best Practices</li> <li>\ud83d\udea8 Common Issues</li> </ul>"},{"location":"extending/#multi-agent-system-extension","title":"\ud83e\udd16 Multi-Agent System Extension","text":""},{"location":"extending/#implementation-requirements","title":"\ud83d\udccb Implementation Requirements","text":"<p>\u2705 Essential Requirements: - Extend <code>AgentSystem</code> base class - Implement <code>run_agent()</code> method (abstract method - required) - Include <code>evaluator</code> in config during initialization - Return proper message format with usage metadata - Register with <code>AgentSystemRegistry</code></p> <p>\ud83d\udca1 Optional but Recommended: - Implement <code>_create_agents()</code> for tool integration support - Use <code>self.format_prompt</code> for benchmark-specific formatting - Handle async execution properly if needed</p>"},{"location":"extending/#implementation-steps","title":"\ud83d\udcdd Implementation Steps","text":""},{"location":"extending/#step-1-create-agent-system-class-structure","title":"Step 1: Create Agent System Class Structure","text":"<p>\u2705 Langgraph supported \u2705 Customizable agent and multi-agent interaction </p> <p>\ud83d\udccb Implementation Guide:    - Inherit from <code>AgentSystem</code> base class    - Initialize configuration parameters (num_agents, num_rounds, model_name)    - Set up agent components using <code>_create_agents()</code> method    - Extract workers and result extractors from created components    - Validate that required components are available</p> <p>\ud83d\udca1 SupervisorMAS Implementation Example (LangGraph Structure):</p> <pre><code># mas_arena/agents/supervisor_mas.py\n\n    def _init_graph_if_needed(self, problem_input: Optional[Any] = None, feedback: Optional[Any] = None):\n        if self.graph is not None:\n            return\n\n        # _create_agents now returns a dict {\"researcher\": researcher_node, \"coder\": coder_node}\n        # If wrapped by ToolIntegrationWrapper, the nodes will have been modified in-place.\n        worker_nodes_map = self._create_agents(problem_input=problem_input, feedback=feedback)\n\n        research_node_obj = worker_nodes_map.get(\"researcher\")\n        coder_node_obj = worker_nodes_map.get(\"coder\")\n\n        if not research_node_obj or not coder_node_obj:\n            raise RuntimeError(\"Could not find researcher or coder agent nodes from _create_agents dictionary.\")\n\n        builder = StateGraph(State)\n        checkpointer = InMemorySaver()\n\n        supervisor_model = self.config.get(\"supervisor_model_name\", self.config.get(\"model_name\", os.getenv(\"MODEL_NAME\", \"gpt-4o-mini\")))\n        builder.add_node(\"supervisor\", create_supervisor(model_name=supervisor_model))\n\n        builder.add_node(\"researcher\", research_node_obj)\n        builder.add_node(\"coder\", coder_node_obj)\n\n        builder.add_edge(START, \"supervisor\")\n\n        builder.add_conditional_edges(\n            \"supervisor\",\n            lambda x: x[\"next\"],\n            {\"researcher\": \"researcher\", \"coder\": \"coder\", END: END},\n        )\n\n        builder.add_edge(\"researcher\", \"supervisor\")\n        builder.add_edge(\"coder\", \"supervisor\")\n\n        self.graph = builder.compile(checkpointer=checkpointer)\n</code></pre> <p>\ud83d\udca1 ChatEval Implementation Example (Basic Structure): <pre><code># mas_arena/agents/chateval.py\nclass ChatEval(AgentSystem):\n    \"\"\"Multi-agent evaluation system based on iterative debate\"\"\"\n\n    def __init__(self, name: str = \"chateval\", config: Dict[str, Any] = None):\n        super().__init__(name, config)\n        self.config = config or {}\n        self.num_agents = self.config.get(\"num_agents\", 3)\n        self.num_rounds = self.config.get(\"num_rounds\", 2)\n        self.model_name = self.config.get(\"model_name\") or os.getenv(\"MODEL_NAME\", \"gpt-4o-mini\")\n\n        # Initialize agents and extractor via _create_agents\n        # self.agents and self.extractor will be set by _create_agents\n        agent_components = self._create_agents()\n        self.agents = [w for w in agent_components[\"workers\"] if isinstance(w, Agent)]\n        extractors = [w for w in agent_components[\"workers\"] if isinstance(w, ResultExtractor)]\n        if not extractors:\n            raise ValueError(\"ResultExtractor not found in components created by _create_agents.\")\n        self.extractor = extractors[0]\n</code></pre></p>"},{"location":"extending/#step-2-implement-core-run_agent-method","title":"Step 2: Implement Core <code>run_agent</code> Method","text":"<p>\ud83d\udccb Implementation Guide:    - Extract problem text from input dictionary    - Initialize message storage for tracking LLM responses    - Implement multi-round agent interaction logic    - Collect and process agent responses with proper metadata    - Extract final answer using result extractor    - Return formatted result with messages and final answer</p> <p>\ud83d\udca1 ChatEval Implementation Example (run_agent Core Method): <pre><code># mas_arena/agents/chateval.py\n    async def run_agent(self, problem: Dict[str, Any], **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"Run iterative debate process\"\"\"\n        problem_text = problem[\"problem\"]\n\n        # store all LLM response objects\n        all_messages = []\n        agent_histories = []\n\n        # iterative discussion process\n        agent_names = [\"Math Expert\", \"Logic Expert\", \"Critical Thinking Expert\"]\n        for t in range(self.num_rounds):\n            for n, agent in enumerate(self.agents):\n                # generate response for current agent\n                context = self._build_context(problem_text, n, t)\n                response_data = agent.generate_response(context)\n\n                # save response object\n                if \"message\" in response_data:\n                    all_messages.append(response_data[\"message\"])\n\n                # add response to context of subsequent agents\n                solution_text = response_data.get(\"solution\", \"\")\n                for m in range(n + 1, len(self.agents)):\n                    self.agents[m].chat_history.append({\n                        \"role\": \"human\",\n                        \"human\": f\"{agent_names[n]}'s response: {solution_text}\"\n                    })\n\n        # extract all agent chat histories\n        agent_histories = [agent.chat_history for agent in self.agents]\n\n        # extract final answer\n        extractor_result = self.extractor.extract(agent_histories, problem_text)\n\n        # add evaluator message\n        if \"message\" in extractor_result and extractor_result[\"message\"]:\n            all_messages.append(extractor_result[\"message\"])\n        return {\n            \"messages\": all_messages,  # contains all LLM response objects\n            \"final_answer\": extractor_result[\"message\"].content\n        }\n</code></pre></p>"},{"location":"extending/#step-3-implement-_create_agents-method-tool-integration-support","title":"Step 3: Implement <code>_create_agents</code> Method (Tool Integration Support)","text":"<p>\ud83d\udccb Implementation Guide:    - Create specialized <code>AgentNode</code> instances for each role    - Set agent names, models, and system prompts    - Create result extractor with format prompt integration    - Return dictionary with \"workers\" key containing all components    - Ensure each worker has <code>.name</code> and <code>.llm</code> attributes for tool binding</p> <p>\ud83d\udca1 ChatEval Implementation Example (_create_agents Tool Integration): <pre><code># mas_arena/agents/chateval.py\n    def _create_agents(self) -&gt; List[Agent]:\n        \"\"\"Create multiple agent instances and result extractor\"\"\"\n        # This method will be patched by ToolIntegrationWrapper if this system is wrapped.\n        # The wrapper expects a dictionary: {\"workers\": [worker1, worker2, ...]}\n        # Each worker should have a .name and .llm attribute.\n\n        debate_agents = []\n        agent_names = [\"Math Expert\", \"Logic Expert\", \"Critical Thinking Expert\"]\n        for i in range(self.num_agents):\n            agent = Agent(\n                agent_id=f\"agent_{i+1}\",\n                name=agent_names[i],\n                model_name=self.model_name,\n                system_prompt=self._get_agent_prompt(i)\n            )\n            debate_agents.append(agent)\n\n        # Create and assign the extractor here\n        extractor = ResultExtractor(self.model_name, self.format_prompt)\n        # self.extractor = extractor # Assign to self if needed elsewhere before run_agent completes,\n                                 # but __init__ already handles setting self.extractor.\n\n        return {\n            \"workers\": debate_agents + [extractor]\n        }\n</code></pre></p>"},{"location":"extending/#step-4-register-system-with-framework","title":"Step 4: Register System with Framework","text":"<p>\ud83d\udccb Implementation Guide:    - Use <code>AgentSystemRegistry.register()</code> to make system available    - Provide system name as string identifier    - Pass class reference (not instance)    - Include default configuration parameters    - These defaults can be overridden during initialization</p> <p>\ud83d\udca1 ChatEval Implementation Example (Registration): <pre><code># mas_arena/agents/chateval.py\n# register agent system\nAgentSystemRegistry.register(\n    \"chateval\",\n    ChatEval,\n    num_agents=3,\n    num_rounds=2\n)\n</code></pre></p>"},{"location":"extending/#advanced-features","title":"\u26a1 Advanced Features","text":""},{"location":"extending/#format-prompt-integration","title":"\ud83c\udfa8 Format Prompt Integration","text":"<p>\ud83d\udccb Implementation Guide:    - Accept <code>format_prompt</code> parameter in initialization    - Store format prompt for benchmark-specific requirements    - Use format prompt in result extraction and agent prompts    - Configure timeout and retry settings for robust operation</p> <p>\ud83d\udca1 ChatEval Implementation Example (Format Prompt Integration): <pre><code># mas_arena/agents/chateval.py\n    def __init__(self, model_name: str = None, format_prompt: str = \"\"):\n        self.model_name = model_name or os.getenv(\"MODEL_NAME\", \"gpt-4o-mini\")\n        self.format_prompt = format_prompt\n        self.llm = ChatOpenAI(\n            model=self.model_name,\n            request_timeout=60,  # Set request timeout to 60 seconds\n            max_retries=2        # Set maximum retry attempts to 2\n        )\n        self.name = \"result_extractor\"\n</code></pre></p>"},{"location":"extending/#agent-node-pattern","title":"\ud83e\udd16 Agent Node Pattern","text":"<p>\ud83d\udccb Implementation Guide:    - Use dataclass decorator for clean agent definition    - Include required attributes: agent_id, name, model_name, system_prompt    - Initialize chat history as empty list    - Set up LLM instance with timeout and retry configuration    - Ensure compatibility with tool integration framework</p> <p>\ud83d\udca1 ChatEval Implementation Example (Agent Class Definition): <pre><code># mas_arena/agents/chateval.py\n@dataclass\nclass Agent:\n    \"\"\"Represents an LLM agent\"\"\"\n    agent_id: str\n    name: str\n    model_name: str\n    system_prompt: str\n    chat_history: List[Dict[str, str]] = None\n\n    def __post_init__(self):\n        self.chat_history = []\n        self.llm = ChatOpenAI(\n            model=self.model_name,\n            request_timeout=60,  # Set request timeout to 60 seconds\n            max_retries=2        # Set maximum retry attempts to 2\n        )\n</code></pre></p>"},{"location":"extending/#usage-metadata-handling","title":"\ud83d\udd04 Usage Metadata Handling","text":"<p>\ud83d\udccb Implementation Guide:    - For native OpenAI API calls or non-structured output: No manual handling required    - For structured output: Use <code>self.llm.with_structured_output(schema=AgentResponse, include_raw=True)</code>    - Usage metadata is automatically handled by the framework    - Focus on implementing the structured output schema instead</p>"},{"location":"extending/#key-implementation-summary","title":"\ud83d\udccb Key Implementation Summary","text":"<p>\ud83d\udd27 Implementation Points: - Inherit from <code>AgentSystem</code> base class - Implement required <code>run_agent()</code> method - Ensure config includes <code>evaluator</code> key - Return dictionary containing <code>messages</code> and <code>final_answer</code> - Optional: Implement <code>_create_agents()</code> for tool integration support</p> <p>\ud83d\udcdd Registration Process: Use <code>AgentSystemRegistry.register()</code> to register system and provide default configuration parameters.</p> <p>\ud83d\udcc4 Complete Implementation Reference: <code>mas_arena/agents/chateval.py</code></p>"},{"location":"extending/#evaluator-extension","title":"\ud83c\udfaf Evaluator Extension","text":""},{"location":"extending/#basic-implementation","title":"\ud83d\udd27 Basic Implementation","text":""},{"location":"extending/#step-1-basic-structure-and-registration","title":"Step 1: Basic Structure and Registration","text":"<p>\ud83d\udccb Implementation Guide:    - Use <code>@register_benchmark</code> decorator to register evaluator    - Define normalization keys mapping for data field standardization    - Inherit from <code>BaseEvaluator</code> base class    - Provide comprehensive docstring explaining evaluator purpose    - Set up evaluator name and supported answer formats</p> <p>\ud83d\udca1 MMLU_pro Implementation Example (Registration and Class Definition): <pre><code># mas_arena/evaluators/mmlu_pro_evaluator.py\n@register_benchmark(\n    name=\"mmlu_pro\",\n    normalization_keys={\n        \"id\": \"id\",\n        \"problem\": \"question\",\n        \"solution\": \"answer\",\n    }\n)\nclass MMLU_ProEvaluator(BaseEvaluator):\n    \"\"\"\n    Evaluator for the MMLU Professional mas_arena.\n\n    This evaluator assesses agent performance on the MMLU_pro dataset\n    using exact matching of answers (A, B, C, etc.).\n    \"\"\"\n</code></pre></p>"},{"location":"extending/#step-2-initialize-configuration","title":"Step 2: Initialize Configuration","text":"<p>\ud83d\udccb Implementation Guide:    - Call parent class initialization with name and config    - Set up evaluation-specific weights and parameters    - Configure dataset loading and validation    - Set up logging and error handling    - Define evaluation metrics and scoring methods</p> <p>\ud83d\udca1 MMLU_pro Implementation Example (Initialization): <pre><code># mas_arena/evaluators/mmlu_pro_evaluator.py\n    def __init__(self, name=\"mmlu_pro\", config=None):\n        \"\"\"\n        Initialize the MMLU Professional evaluator.\n\n        Args:\n            name: Name of the evaluator\n            config: Configuration dictionary containing:\n                - data_path: Path to the MMLU_pro dataset\n                - log_path: Path to save evaluation logs\n        \"\"\"\n        super().__init__(name, config or {})\n\n        # Weight for exact match score is always 1.0 as it's the only metric\n        self.exact_match_weight = 1.0\n\n        # Load the dataset\n        self._load_dataset()\n</code></pre></p>"},{"location":"extending/#step-3-implement-core-evaluation-method","title":"Step 3: Implement Core Evaluation Method","text":"<p>\ud83d\udccb Implementation Guide:    - Extract final answer and reference solution from inputs    - Use specialized answer extraction method for response parsing    - Apply scoring logic (exact match, numerical comparison, etc.)    - Calculate evaluation metrics and scores    - Return standardized evaluation results dictionary    - Include extracted answer and original final answer</p> <p>\ud83d\udca1 MMLU_pro Implementation Example (evaluate Method): <pre><code># mas_arena/evaluators/mmlu_pro_evaluator.py\n    def evaluate(self, problem: Dict[str, Any], run_result: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate an agent's solution to a MMLU_pro problem.\n\n        Args:\n            problem: Problem dictionary containing:\n                - question: Problem text (with options)\n                - answer: Correct answer (letter)\n                - answer_index: Index of correct answer (optional)\n            run_result: Results from agent's execution, containing:\n                - final_answer: Agent's final answer text\n                - messages: Agent's message history\n\n        Returns:\n            Evaluation results\n        \"\"\"\n        final_answer = run_result.get(\"final_answer\", \"\")\n        reference_letter = problem.get(\"solution\", \"\")\n\n        # Extract the final letter from the agent's response\n        extracted_answer = self.extract_answer_from_response(final_answer)\n\n        # Calculate exact match score (letter-based)\n        score = self.check_exact_match(reference_letter, extracted_answer)\n\n        # Record evaluation results\n        return {\n            \"final_answer\": final_answer,\n            \"extracted_answer\": extracted_answer,\n            \"score\": score,\n        }\n</code></pre></p>"},{"location":"extending/#advanced-features_1","title":"\u26a1 Advanced Features","text":""},{"location":"extending/#answer-extraction","title":"\ud83d\udd0d Answer Extraction","text":"<p>\ud83d\udccb Implementation Guide:    - Use regular expressions to extract formatted answers    - Handle multiple answer formats (tags, patterns, raw text)    - Implement fallback strategies for unformatted responses    - Clean and normalize extracted text    - Support flexible answer parsing for different benchmarks</p> <p>\ud83d\udca1 MMLU_pro Implementation Example (Answer Extraction): <pre><code># mas_arena/evaluators/mmlu_pro_evaluator.py\n    def extract_answer_from_response(self, response: str) -&gt; str:\n        \"\"\"\n        Extract answer from agent response.\n\n        Args:\n            response: Complete response text from agent\n\n        Returns:\n            Extracted answer letter\n        \"\"\"\n        # Try to extract answer from &lt;answer&gt; tags, allowing for whitespace\n        match = re.search(r'&lt;answer&gt;\\s*(.*?)\\s*&lt;/answer&gt;', response, re.DOTALL)\n        if match:\n            return match.group(1).strip()\n\n        # If no tags found, return original response\n        return response.strip()\n</code></pre></p>"},{"location":"extending/#answer-verification","title":"\u2705 Answer Verification","text":"<p>\ud83d\udccb Implementation Guide:    - Implement case-insensitive comparison for text answers    - Handle numerical index to letter conversion (1\u2192A, 2\u2192B, etc.)    - Apply normalization and cleaning to both reference and candidate    - Return numerical score (1.0 for match, 0.0 for no match)    - Include error handling for malformed inputs</p> <p>\ud83d\udca1 MMLU_pro Implementation Example (Exact Match Verification): <pre><code># mas_arena/evaluators/mmlu_pro_evaluator.py\n    def check_exact_match(self, reference: str, candidate: str) -&gt; float:\n        \"\"\"\n        Check if the candidate exactly matches the reference (case-insensitive).\n\n        Args:\n            reference: Reference answer (e.g., 'A', 'B', 'C', etc.)\n            candidate: Candidate answer\n\n        Returns:\n            1.0 if exact match, 0.0 otherwise\n        \"\"\"\n        # Clean and normalize both answers\n        ref_clean = reference.strip().upper()\n        cand_clean = candidate.strip().upper()\n\n        # Check for exact match\n        if cand_clean == ref_clean:\n            return 1.0\n\n        # Check if candidate is an index (e.g., \"1\", \"2\", \"3\") converted to letter\n        try:\n            if cand_clean.isdigit():\n                cand_index = int(cand_clean) - 1\n                cand_letter = chr(ord('A') + cand_index)\n                if cand_letter == ref_clean:\n                    return 1.0\n        except Exception:\n            pass\n\n        return 0.0\n</code></pre></p>"},{"location":"extending/#batch-evaluation","title":"\ud83d\udcca Batch Evaluation","text":"<p>\ud83d\udccb Implementation Guide:    - Iterate through all problems in the batch    - Extract problem IDs and reference answers for each item    - Apply evaluation logic consistently across all problems    - Collect comprehensive results with metadata    - Log evaluation progress and summary statistics    - Return standardized results format for benchmark runner</p> <p>\ud83d\udca1 MMLU_pro Implementation Example (Batch Evaluation): <pre><code># mas_arena/evaluators/mmlu_pro_evaluator.py\n    def batch_evaluate(self, problems: List[Dict[str, Any]], **kwargs) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Evaluate a batch of problems.\n\n        Args:\n            problems: List of problem dictionaries\n\n        Returns:\n            List of evaluation results\n        \"\"\"\n        results = []\n\n        # Evaluate each problem individually\n        for i, problem in enumerate(problems):\n            problem_id = problem.get(\"id\", problem.get(\"question_id\", f\"unknown_{i}\"))\n            reference_letter = problem.get(\"solution\", problem.get(\"answer\", \"\"))\n            reference_text = self.get_correct_answer_text(problem)\n            response = problem.get(\"response\", \"\")\n\n            # Calculate exact match score\n            exact_match = self.check_exact_match(reference_letter, response)\n\n            # Record results\n            result = {\n                \"problem_id\": problem_id,\n                \"exact_match\": exact_match,\n                \"combined_score\": exact_match,  # Combined score is just the exact match\n                \"extracted_answer\": response,\n                \"reference_answer\": reference_letter,\n                \"reference_text\": reference_text,\n                \"execution_time_ms\": 0,  # Will be updated by the benchmark runner\n                \"math_score\": 1.0 if exact_match &gt;= 0.9 else 0.0  # For compatibility with benchmark runner\n            }\n\n            results.append(result)\n\n            # Log the results\n            self.logger.info(f\"Problem {problem_id}: Exact={exact_match:.1f}, Combined={exact_match:.4f}\")\n\n        return results\n</code></pre></p>"},{"location":"extending/#code-evaluation","title":"\ud83d\udcbb Code Evaluation","text":"<p>\ud83d\udd27 Code Evaluator Key Points: - Inherit from <code>BaseCodeEvaluator</code> base class (not BaseEvaluator) - Implement <code>check_solution(code, test, entry_point)</code> method - Implement <code>extract_code(text)</code> to extract code from responses - Must include timeout protection mechanisms - Use isolated environments for code execution</p> <p>\ud83d\udcca Core Process Flow: 1. Code Extraction - Extract Python code from agent responses 2. Environment Isolation - Create secure execution environment 3. Test Execution - Run test cases to verify code correctness 4. Timeout Control - Prevent infinite loops or long execution</p>"},{"location":"extending/#evaluator-implementation-summary","title":"\ud83d\udccb Evaluator Implementation Summary","text":"<p>\ud83d\udd27 Core Components: - Use <code>@register_benchmark</code> decorator for registration - Inherit from <code>BaseEvaluator</code> base class - Implement required <code>evaluate()</code> method - Configure <code>normalization_keys</code> for data mapping - Optional: Implement answer extraction and verification methods</p> <p>\ud83d\udcca Evaluation Process: 1. Data Normalization - Map fields using normalization_keys 2. Answer Extraction - Extract final answer from messages 3. Answer Verification - Compare predicted vs reference answers 4. Result Return - Return score, extracted_answer, final_answer fields</p> <p>\ud83d\udcc4 Complete Implementation References:  - Text Evaluator: <code>mas_arena/evaluators/mmlu_pro_evaluator.py</code> - Code Evaluator: <code>mas_arena/evaluators/humaneval_evaluator.py</code></p>"},{"location":"extending/#best-practices","title":"\u2705 Best Practices","text":""},{"location":"extending/#performance-security","title":"\ud83d\ude80 Performance &amp; Security","text":"<ul> <li>\u26a1 Batch Processing: Implement <code>batch_evaluate()</code> for better performance</li> <li>\u23f1\ufe0f Timeout Handling: Always set timeouts for external calls and code execution</li> <li>\ud83d\udd0d Input Validation: Validate all inputs before processing</li> <li>\ud83d\udee1\ufe0f Error Handling: Implement comprehensive exception handling</li> <li>\ud83d\udcdd Logging: Add detailed logging for debugging and monitoring</li> </ul>"},{"location":"extending/#testing-validation","title":"\ud83e\uddea Testing &amp; Validation","text":"<ul> <li>\ud83c\udfaf Unit Tests: Test individual components thoroughly</li> <li>\ud83d\udd04 Integration Tests: Test full evaluation pipeline</li> <li>\u26a0\ufe0f Edge Cases: Test with malformed inputs and edge cases</li> <li>\ud83d\udcca Performance Tests: Benchmark evaluation speed for large datasets</li> </ul>"},{"location":"extending/#common-issues","title":"\ud83d\udea8 Common Issues","text":""},{"location":"extending/#implementation-checklist","title":"\ud83d\udccb Implementation Checklist","text":"<p>For MAS Extensions: - [ ] \u2705 Config includes <code>evaluator</code> key - [ ] \ud83d\udcca Messages have <code>usage_metadata</code> for token tracking - [ ] \ud83c\udff7\ufe0f Agents have <code>name</code> and <code>llm</code> attributes (for tool integration) - [ ] \u26a1 <code>run_agent</code> should be async - [ ] \ud83d\udce4 Return format includes <code>messages</code> and <code>final_answer</code> - [ ] \ud83d\udccb Proper registration with <code>AgentSystemRegistry</code></p> <p>For Evaluator Extensions: - [ ] \ud83c\udfaf Used <code>@register_benchmark</code> decorator - [ ] \u2705 Implemented <code>evaluate</code> method - [ ] \ud83d\udddd\ufe0f Proper normalization_keys mapping - [ ] \ud83d\udee1\ufe0f Error handling for malformed inputs - [ ] \u23f1\ufe0f Timeout handling for long operations</p>"},{"location":"failure_attribution/","title":"Failure Attribution Module","text":"<p>This module provides automated failure attribution capabilities for analyzing multi-agent system responses and identifying failure causes. The module has been migrated and adapted from the <code>Automated_FA</code>.</p>"},{"location":"failure_attribution/#overview","title":"Overview","text":"<p>The failure attribution module analyzes agent conversation histories to identify: - Which agent made an error - At which step the error occurred - What type of error it was - The specific reason for the failure</p>"},{"location":"failure_attribution/#usage","title":"Usage","text":"<p>The module can be run using the <code>inference.py</code> script with various analysis methods:</p> <pre><code>python inference.py --method all_at_once --model gpt-4o --directory_path ../results/agent_responses\n</code></pre>"},{"location":"failure_attribution/#analysis-methods","title":"Analysis Methods","text":"<ol> <li> <p>All-at-once Analysis: Analyzes the entire conversation history at once <pre><code>python inference.py --method all_at_once --model gpt-4o\n</code></pre></p> </li> <li> <p>Step-by-step Analysis: Analyzes the conversation incrementally, step by step <pre><code>python inference.py --method step_by_step --model gpt-4o\n</code></pre></p> </li> <li> <p>Binary Search Analysis: Uses binary search to efficiently locate errors <pre><code>python inference.py --method binary_search --model gpt-4o\n</code></pre></p> </li> </ol>"},{"location":"failure_attribution/#output","title":"Output","text":"<p>The analysis results are saved to the <code>outputs/</code> directory with filenames in the format: <code>{method}_{model}_agent_responses.txt</code></p> <p>Example output format: <pre><code>Error Agent: agent_2\nError Step: 3\nError Type: Calculation Error\nReason: The agent made an arithmetic error in the calculation step.\n</code></pre></p>"},{"location":"failure_attribution/#data-format-agent_responses","title":"Data Format (agent_responses)","text":"<ul> <li>Uses <code>responses</code> field for agent interactions</li> <li>No ground truth labels (unsupervised analysis)</li> <li>Includes <code>problem_id</code> and <code>agent_system</code> metadata</li> <li>Each response contains <code>agent_id</code>, <code>content</code>, and <code>timestamp</code></li> </ul> <pre><code>{\n    \"problem_id\": \"problem_1\",\n    \"agent_system\": \"multi_agent\",\n    \"run_id\": \"run_123\",\n    \"timestamp\": \"2024-06-24T16:11:44\",\n    \"responses\": [\n        {\n            \"timestamp\": \"2024-06-24T16:11:44.123\",\n            \"problem_id\": \"problem_1\",\n            \"message_index\": 0,\n            \"agent_id\": \"agent_1\",\n            \"content\": \"Agent response content here...\",\n            \"role\": \"assistant\",\n            \"message_type\": \"response\",\n            \"usage_metadata\": {}\n        }\n    ]\n}\n</code></pre>"},{"location":"failure_attribution/#evaluation","title":"Evaluation","text":"<p>To evaluate the accuracy of failure attribution predictions:</p> <pre><code>python evaluate.py --data_path /path/to/annotated/data --evaluation_file outputs/all_at_once_gpt-4o_agent_responses.txt\n</code></pre> <p>The evaluation script compares predictions against ground truth annotations and reports: - Agent identification accuracy - Error step identification accuracy</p>"},{"location":"failure_attribution/#troubleshooting","title":"Troubleshooting","text":""},{"location":"failure_attribution/#performance-tips","title":"Performance Tips","text":"<ul> <li>Use <code>all_at_once</code> for comprehensive analysis</li> <li>Use <code>binary_search</code> for efficient error localization in long conversations</li> <li>Use <code>step_by_step</code> for detailed incremental analysis</li> <li>For local models, ensure adequate GPU memory or use CPU inference</li> </ul>"},{"location":"roadmap/","title":"Project Roadmap","text":"<p>This document outlines the planned development timeline and future direction for the Multi-Agent System Arena (MAS-Arena). Our goal is to evolve this framework into a comprehensive and community-driven platform for multi-agent research and evaluation.</p>"},{"location":"roadmap/#v010","title":"v0.1.0","text":"<p>Focus: Initial public release with core framework for evaluating and comparing single and multi-agent systems.</p> Category Key Initiatives Core Framework - Modular Design: Key components like agents, tools, datasets, prompts, and evaluators are designed to be swappable. - Straightforward to add new benchmarks with paired datasets and evaluators. Benchmarks - Built-in Benchmarks: Integrated several benchmarks for direct comparison of agent systems. - Supported Benchmarks: <code>math</code>, <code>aime</code>, <code>humaneval</code>, <code>mbpp</code>, <code>drop</code>, <code>bbh</code>, <code>mmlu_pro</code>, <code>ifeval</code>. Agent Systems - Supported Agent Systems: Includes a single agent baseline and various multi-agent systems: <code>single_agent</code>, <code>supervisor_mas</code>, <code>swarm</code>, <code>agentverse</code>, <code>chateval</code>, <code>evoagent</code>, <code>jarvis</code>, <code>metagpt</code>. Tooling - Pluggable Tool Support: A wrapper-based system to manage tool selection and integration for agents. Visualization - Visual Debugging: Support for inspecting agent interactions, accuracy, and tool usage."},{"location":"roadmap/#v020","title":"v0.2.0","text":"<p>Focus: Strengthen the existing foundation, improve developer experience, and expand core features based on initial feedback.</p> Category Key Initiatives Core Framework - Configuration Overhaul: Introduce configuration (e.g., using YAML/Hydra) to simplify managing complex agent and benchmark settings.  - Failure Analysis: Add a failure analysis plugin to help identify why MAS fails. Benchmarks - Enhance with tools: Integrate multiple tools(e.g., Browser, Video, Audio, Docker) and benchmarks for tool usage, like <code>swebench</code>. Agent Systems - Continuously Integrate New Agent Systems.- Standardize Agent Outputs: Enforce a more standardized output schema for <code>run_agent</code> to simplify evaluation logic. Tooling - Optimize tool management architecture: Decouple MCP tool invocation from local tool invocation. - Tool Caching: Implement a caching layer for tool outputs. Documentation &amp; Community -Tutorials: Write step by step tutorials on \"Adding a New Agent\" and \"Creating a Custom Benchmark\"."},{"location":"roadmap/#v030","title":"v0.3.0","text":"<p>Focus: Add more complex benchmarks, integrate a wider variety of agent systems, and grow the tool ecosystem.</p> Category Key Initiatives Core Framework - Agent-to-Agent Communication: Develop a standardized internal message-passing. - Sandbox Environment: Enhance the execution environment with finer-grained control. Tooling - Tool Discovery &amp; Creation: Develop an experimental feature where agents can attempt to generate new tools (e.g., Python functions) on the fly and add them to their context.- Async Tools: Refactor the <code>ToolManager</code> to fully support asynchronous tool execution."},{"location":"supported/","title":"Supported","text":""},{"location":"supported/#supported-benchmarks","title":"\ud83d\udcca Supported Benchmarks","text":"Benchmark Description Dataset File <code>math</code> Mathematical problem solving <code>math_test.jsonl</code> <code>humaneval</code> Python code generation <code>humaneval_test.jsonl</code> <code>mbpp</code> Python programming problems <code>mbpp_test.jsonl</code> <code>drop</code> Reading comprehension <code>drop_test.jsonl</code> <code>bbh</code> Complex reasoning tasks <code>bbh_test.jsonl</code> <code>ifeval</code> Instruction following <code>ifeval_test.jsonl</code> <code>aime</code> Math competition problems <code>aime_*_test.jsonl</code> <code>mmlu_pro</code> Multi-domain knowledge <code>mmlu_pro_test.jsonl</code>"},{"location":"supported/#supported-agent-systems","title":"\ud83e\udd16 Supported Agent Systems","text":"Agent System File Description <code>single_agent</code> <code>single_agent.py</code> Single LLM agent <code>supervisor_mas</code> <code>supervisor_mas.py</code> Supervisor-based multi-agent system <code>swarm</code> <code>swarm.py</code> Swarm-based agent system <code>agentverse</code> <code>agentverse.py</code> Dynamic recruitment agent system <code>chateval</code> <code>chateval.py</code> Debate-based multi-agent system <code>evoagent</code> <code>evoagent.py</code> Evolutionary agent system <code>jarvis</code> <code>jarvis.py</code> Task-planning agent system <code>metagpt</code> <code>metagpt.py</code> Code generation agent system"},{"location":"system_overview/","title":"System Architecture","text":"<p>This document provides a detailed overview of the MAS Arena system's architecture. It explains the core components, their interactions, and the overall data flow when running a benchmark.</p>"},{"location":"system_overview/#high-level-architecture","title":"High-Level Architecture","text":"<p>The system is designed to be modular and extensible, allowing for easy addition of new agent systems and benchmarks. The core components are the <code>BenchmarkRunner</code>, <code>AgentSystem</code>, and <code>Evaluator</code>. The <code>BenchmarkRunner</code> orchestrates the process, while the <code>AgentSystem</code> encapsulates the logic for both solving a problem and evaluating its own solution.</p> <pre><code>graph TD\n    subgraph User Interaction\n        A[run_benchmark.sh]\n    end\n\n    subgraph Core Orchestration\n        B[main.py]\n        C[BenchmarkRunner]\n    end\n\n    subgraph Agent System Abstraction\n        D[agents.create_agent_system]\n        E[agents.AgentSystemRegistry]\n        F[agents.base.AgentSystem]\n        G[agents.run_agent]\n    end\n\n    subgraph Concrete Agent Systems\n        direction LR\n        H[MetaGPT]\n        I[AgentVerse]\n        J[Swarm]\n        K[...]\n    end\n\n    subgraph Tool Abstraction\n        U[tools.base.BaseTool]\n        V[tools.ToolManager]\n        W[tools.ToolSelector]\n    end\n\n    subgraph Concrete Tools\n        direction LR\n        X[ShellTool]\n        Y[BrowserTool]\n        Z[PythonExecuteTool]\n        AA[...]\n    end\n\n    subgraph Evaluator Abstraction\n        L[evaluators.base_evaluator.BaseEvaluator]\n        M[evaluators.evaluate]\n    end\n\n    subgraph Concrete Evaluators\n        direction LR\n        N[HumanEvalEvaluator]\n        O[MBPPEvaluator]\n        P[MathEvaluator]\n        Q[...]\n    end\n\n    subgraph Data\n        R[Benchmark Datasets]\n    end\n\n    subgraph Results\n        S[Results]\n        T[Metrics]\n    end\n\n    A -- \"Executes with args (agent, benchmark)\" --&gt; B\n    B -- \"Instantiates &amp; calls\" --&gt; C\n\n    C -- \"Calls with agent name\" --&gt; D\n    D -- \"Looks up in\" --&gt; E\n    E -- \"Instantiates\" --&gt; F\n\n    F -- \"Is subclassed by\" --&gt; H\n    F -- \"Is subclassed by\" --&gt; I\n    F -- \"Is subclassed by\" --&gt; J\n    F -- \"Is subclassed by\" --&gt; K\n\n    F -- \"Uses\" --&gt; V\n\n    V -- \"Uses\" --&gt; W\n    W -- \"Selects from\" --&gt; U\n\n    U -- \"Is subclassed by\" --&gt; X\n    U -- \"Is subclassed by\" --&gt; Y\n    U -- \"Is subclassed by\" --&gt; Z\n    U -- \"Is subclassed by\" --&gt; AA\n\n    F -- \"Initializes\" --&gt; L\n    L -- \"Is subclassed by\" --&gt; N\n    L -- \"Is subclassed by\" --&gt; O\n    L -- \"Is subclassed by\" --&gt; P\n    L -- \"Is subclassed by\" --&gt; Q\n\n    C -- \"Loads\" --&gt; R\n    C -- \"For each problem in dataset, calls\" --&gt; F\n\n    F -- \"evaluate(problem) calls\" --&gt; G\n    G -- \"Gets result, then calls\" --&gt; M\n\n    C -- \"Saves\" --&gt; S\n    C -- \"Saves\" --&gt; T\n\n    style F fill:#f9f,stroke:#333,stroke-width:2px\n    style L fill:#ccf,stroke:#333,stroke-width:2px\n    style U fill:#ffc,stroke:#333,stroke-width:2px</code></pre>"},{"location":"system_overview/#execution-workflow","title":"Execution Workflow","text":"<p>The following sequence diagram illustrates the step-by-step workflow when a benchmark is executed. A key design choice is that the <code>AgentSystem</code> is responsible for its own evaluation. It creates an appropriate <code>Evaluator</code> during its initialization and uses it to score the solutions it generates.</p> <pre><code>sequenceDiagram\n    participant main.py\n    participant BenchmarkRunner\n    participant AgentSystem\n    participant ToolManager\n    participant Evaluator\n\n    main.py-&gt;&gt;BenchmarkRunner: Instantiate &amp; run(...)\n\n    BenchmarkRunner-&gt;&gt;AgentSystem: create_agent_system(...)\n    activate AgentSystem\n\n    AgentSystem-&gt;&gt;ToolManager: Instantiate()\n    activate ToolManager\n    Note over AgentSystem, ToolManager: AgentSystem creates a ToolManager\n    deactivate ToolManager\n\n    AgentSystem-&gt;&gt;Evaluator: Instantiate(...)\n    activate Evaluator\n    Note over AgentSystem, Evaluator: AgentSystem creates an Evaluator\n    deactivate Evaluator\n\n    BenchmarkRunner--&gt;&gt;AgentSystem: Return agent instance\n\n    loop For each problem in dataset\n        BenchmarkRunner-&gt;&gt;AgentSystem: evaluate(problem)\n\n        AgentSystem-&gt;&gt;AgentSystem: run_agent(problem)\n        activate AgentSystem\n\n        Note right of AgentSystem: Core agent logic starts\n\n        AgentSystem-&gt;&gt;ToolManager: execute_tool(tool_name, args)\n        activate ToolManager\n        ToolManager--&gt;&gt;AgentSystem: Return tool_output\n        deactivate ToolManager\n\n        Note right of AgentSystem: Agent uses tool output\n\n        deactivate AgentSystem\n\n        AgentSystem-&gt;&gt;Evaluator: evaluate(solution, ground_truth)\n        activate Evaluator\n        Evaluator--&gt;&gt;AgentSystem: Return score &amp; metrics\n        deactivate Evaluator\n\n        AgentSystem--&gt;&gt;BenchmarkRunner: Return evaluation results\n    end\n\n    deactivate AgentSystem\n\n    BenchmarkRunner-&gt;&gt;main.py: Return summary</code></pre>"},{"location":"system_overview/#core-components-decomposition","title":"Core Components Decomposition","text":"<p>The framework's modularity comes from its use of abstract base classes and registries for dynamic discovery.</p>"},{"location":"system_overview/#agent-systems","title":"Agent Systems","text":"<p>All agent systems inherit from the <code>AgentSystem</code> abstract base class. This ensures they conform to a common interface, which includes the <code>run_agent()</code> and <code>evaluate()</code> methods. The <code>AgentSystemRegistry</code> is used to discover and list available agents.</p> <pre><code>classDiagram\n    direction LR\n    class AgentSystem {\n        &lt;&lt;Abstract&gt;&gt;\n        +name: str\n        +config: dict\n        +evaluator: BaseEvaluator\n        +run_agent(problem) dict\n        +evaluate(problem) dict\n    }\n\n    class MetaGPT {\n    }\n\n    class AgentVerse {\n    }\n\n    class Swarm {\n    }\n\n    AgentSystem &lt;|-- MetaGPT\n    AgentSystem &lt;|-- AgentVerse\n    AgentSystem &lt;|-- Swarm</code></pre>"},{"location":"system_overview/#evaluators","title":"Evaluators","text":"<p>Similarly, all evaluators inherit from a <code>BaseEvaluator</code> class (though not strictly enforced as an ABC in the current implementation, it serves this role conceptually). The <code>AVAILABLE_EVALUATORS</code> dictionary in <code>mas_arena/evaluators/__init__.py</code> acts as a registry.</p> <pre><code>classDiagram\n    direction LR\n    class BaseEvaluator {\n        &lt;&lt;Interface&gt;&gt;\n        +name: str\n        +config: dict\n        +evaluate(prediction, expected) dict\n    }\n\n    class HumanEvalEvaluator {\n    }\n\n    class MBPPEvaluator {\n    }\n\n    class MathEvaluator {\n    }\n\n    BaseEvaluator &lt;|-- HumanEvalEvaluator\n    BaseEvaluator &lt;|-- MBPPEvaluator\n    BaseEvaluator &lt;|-- MathEvaluator</code></pre>"},{"location":"system_overview/#extensibility","title":"Extensibility","text":"<p>Adding a new agent or evaluator to the system is straightforward.</p>"},{"location":"system_overview/#adding-a-new-agent","title":"Adding a New Agent","text":"<ol> <li>Create a new Python file in <code>mas_arena/agents/</code>.</li> <li>Implement a new class that inherits from <code>agents.base.AgentSystem</code>.</li> <li>Implement the abstract <code>run_agent()</code> method with the agent's unique logic.</li> <li>Register the new agent in <code>mas_arena/agents/__init__.py</code> by adding it to the <code>AVAILABLE_AGENT_SYSTEMS</code> dictionary and the <code>__all__</code> list.</li> </ol>"},{"location":"system_overview/#adding-a-new-evaluator","title":"Adding a New Evaluator","text":"<ol> <li>Create a new Python file in <code>mas_arena/evaluators/</code>.</li> <li>Implement a new class that provides an <code>evaluate()</code> method.</li> <li>Register the new evaluator in <code>mas_arena/evaluators/__init__.py</code> by adding it to the <code>AVAILABLE_EVALUATORS</code> dictionary.</li> </ol>"},{"location":"quick_start/CONTRIBUTING/","title":"Test Workflow in GitHub Actions","text":""},{"location":"quick_start/CONTRIBUTING/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Testing Framework</li> <li>GitHub Actions CI/CD</li> <li>Running Tests Locally</li> <li>Code Quality Standards</li> <li>Contributing Workflow</li> </ul>"},{"location":"quick_start/CONTRIBUTING/#testing-framework","title":"Testing Framework","text":""},{"location":"quick_start/CONTRIBUTING/#framework-choice","title":"Framework Choice","text":"<p>We use pytest as our primary testing framework because: - Excellent support for async testing with <code>pytest-asyncio</code> - Rich fixture system for test setup and teardown - Comprehensive assertion introspection - Extensive plugin ecosystem - Built-in coverage reporting</p>"},{"location":"quick_start/CONTRIBUTING/#test-structure","title":"Test Structure","text":"<p>Our test suite is organized into the following categories:</p> <pre><code>tests/\n\u251c\u2500\u2500 conftest.py              # Shared fixtures and configuration\n\u251c\u2500\u2500 test_agents.py           # Agent system tests\n\u251c\u2500\u2500 test_evaluators.py       # Evaluator and benchmark tests\n\u2514\u2500\u2500 test_benchmark_runner.py # Benchmark runner integration tests\n</code></pre>"},{"location":"quick_start/CONTRIBUTING/#test-categories","title":"Test Categories","text":"<ol> <li>Unit Tests (<code>@pytest.mark.unit</code>)</li> <li>Test individual functions and classes in isolation</li> <li>Fast execution, no external dependencies</li> <li> <p>Mock external services and APIs</p> </li> <li> <p>Integration Tests (<code>@pytest.mark.integration</code>)</p> </li> <li>Test component interactions</li> <li>May involve file I/O or network calls</li> <li> <p>Use test fixtures and temporary directories</p> </li> <li> <p>Async Tests (<code>@pytest.mark.asyncio</code>)</p> </li> <li>Test asynchronous functionality</li> <li>Agent evaluation and benchmark processing</li> <li>Concurrent execution scenarios</li> </ol>"},{"location":"quick_start/CONTRIBUTING/#key-test-cases","title":"Key Test Cases","text":"<p>Our test suite covers:</p> <ul> <li>Agent Systems: Creation, configuration, and evaluation interfaces</li> <li>Evaluators: Math, GSM8K, and other benchmark evaluators</li> <li>Benchmark Runner: Problem processing, result aggregation, and error handling</li> <li>Registry Systems: Agent and benchmark registration</li> <li>Utilities: JSON serialization, file handling, and metrics collection</li> </ul>"},{"location":"quick_start/CONTRIBUTING/#github-actions-cicd","title":"GitHub Actions CI/CD","text":""},{"location":"quick_start/CONTRIBUTING/#workflow-overview","title":"Workflow Overview","text":"<p>Our CI/CD pipeline (<code>.github/workflows/test.yml</code>) automatically runs on: - Push to <code>main</code> and <code>develop</code> branches - Pull requests targeting <code>main</code> and <code>develop</code> branches</p>"},{"location":"quick_start/CONTRIBUTING/#workflow-jobs","title":"Workflow Jobs","text":""},{"location":"quick_start/CONTRIBUTING/#1-test-job","title":"1. Test Job","text":"<p>Matrix Strategy: Tests run on Python 3.11 and 3.12</p> <p>Steps: 1. Checkout: Get the latest code 2. Python Setup: Install specified Python version 3. Dependency Caching: Cache pip dependencies for faster builds 4. Install Dependencies: Install project and test dependencies 5. Environment Setup: Set <code>PYTHONPATH</code> and test mode flags 6. Unit Tests: Run individual test files with verbose output 7. Coverage Tests: Generate coverage reports 8. Upload Coverage: Send coverage data to Codecov (Python 3.11 only)</p>"},{"location":"quick_start/CONTRIBUTING/#2-lint-job","title":"2. Lint Job","text":"<p>Purpose: Ensure code quality and formatting consistency</p> <p>Steps: 1. Ruff Linting: Check code style and potential issues 2. Format Checking: Verify code formatting (non-blocking)</p>"},{"location":"quick_start/CONTRIBUTING/#caching-strategy","title":"Caching Strategy","text":"<p>We implement dependency caching to improve build performance: - Cache Key: Based on <code>requirements.txt</code> hash - Cache Path: <code>~/.cache/pip</code> - Fallback: OS-specific pip cache</p>"},{"location":"quick_start/CONTRIBUTING/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>PYTHONPATH</code>: Ensures proper module imports</li> <li><code>MAS_ARENA_TEST_MODE</code>: Enables test-specific configurations</li> <li>API keys are mocked in test environment</li> </ul>"},{"location":"quick_start/CONTRIBUTING/#running-tests-locally","title":"Running Tests Locally","text":""},{"location":"quick_start/CONTRIBUTING/#basic-test-execution","title":"Basic Test Execution","text":"<pre><code># Run all tests\npytest\n\n# Run specific test file\npytest tests/test_agents.py -v\n\n# Run tests with coverage\npytest --cov=mas_arena --cov-report=html\n\n# Run only unit tests\npytest -m unit\n\n# Run async tests\npytest -m asyncio\n</code></pre>"},{"location":"quick_start/CONTRIBUTING/#test-configuration","title":"Test Configuration","text":"<p>Our <code>pytest.ini</code> configuration: - Test Discovery: <code>test_*.py</code> files, <code>Test*</code> classes, <code>test_*</code> functions - Markers: <code>slow</code>, <code>integration</code>, <code>unit</code>, <code>asyncio</code> - Warnings: Filtered deprecation warnings - Options: Strict marker enforcement, quiet output</p>"},{"location":"quick_start/CONTRIBUTING/#debugging-tests","title":"Debugging Tests","text":"<pre><code># Verbose output with full traceback\npytest -v --tb=long\n\n# Stop on first failure\npytest -x\n\n# Run specific test method\npytest tests/test_agents.py::TestAgentCreation::test_create_single_agent\n</code></pre>"},{"location":"quick_start/CONTRIBUTING/#code-quality-standards","title":"Code Quality Standards","text":""},{"location":"quick_start/CONTRIBUTING/#linting-with-ruff","title":"Linting with Ruff","text":"<p>We use Ruff for fast Python linting and formatting:</p> <pre><code># Check code style\nruff check mas_arena/\n\n# Format code\nruff format mas_arena/\n\n# Check formatting without changes\nruff format --check mas_arena/\n</code></pre>"},{"location":"quick_start/CONTRIBUTING/#configuration","title":"Configuration","text":"<ul> <li>Line Length: 120 characters</li> <li>Target Version: Python 3.11+</li> <li>Rules: Standard Python style guidelines</li> </ul>"},{"location":"quick_start/CONTRIBUTING/#coverage-requirements","title":"Coverage Requirements","text":"<ul> <li>Maintain test coverage above 80%</li> <li>New features must include corresponding tests</li> <li>Critical paths require comprehensive test coverage</li> </ul>"},{"location":"quick_start/CONTRIBUTING/#contributing-workflow","title":"Contributing Workflow","text":""},{"location":"quick_start/CONTRIBUTING/#before-submitting-a-pull-request","title":"Before Submitting a Pull Request","text":"<ol> <li> <p>Run Tests Locally:    <pre><code>pytest tests/ -v\n</code></pre></p> </li> <li> <p>Check Code Quality:    <pre><code>ruff check mas_arena/\nruff format mas_arena/\n</code></pre></p> </li> <li> <p>Verify Coverage:    <pre><code>pytest --cov=mas_arena --cov-report=term-missing\n</code></pre></p> </li> </ol>"},{"location":"quick_start/CONTRIBUTING/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create Feature Branch: <code>git checkout -b feature/your-feature-name</code></li> <li>Make Changes: Implement your feature with tests</li> <li>Test Locally: Ensure all tests pass</li> <li>Commit Changes: Use descriptive commit messages</li> <li>Push Branch: <code>git push origin feature/your-feature-name</code></li> <li>Create PR: Submit pull request with description</li> <li>CI Validation: Wait for GitHub Actions to pass</li> <li>Code Review: Address reviewer feedback</li> <li>Merge: Squash and merge after approval</li> </ol>"},{"location":"quick_start/CONTRIBUTING/#writing-new-tests","title":"Writing New Tests","text":"<p>When adding new functionality:</p> <ol> <li>Create Test File: Follow naming convention <code>test_*.py</code></li> <li>Use Fixtures: Leverage shared fixtures from <code>conftest.py</code></li> <li>Mock External Dependencies: Use <code>unittest.mock</code> for API calls</li> <li>Test Edge Cases: Include error conditions and boundary cases</li> <li>Add Markers: Use appropriate pytest markers</li> <li>Document Tests: Include docstrings for complex test scenarios</li> </ol>"},{"location":"quick_start/CONTRIBUTING/#example-test-structure","title":"Example Test Structure","text":"<pre><code>\"\"\"Tests for new feature.\"\"\"\n\nimport pytest\nfrom unittest.mock import Mock, patch\n\nfrom mas_arena.your_module import YourClass\n\n\nclass TestYourClass:\n    \"\"\"Test your new class functionality.\"\"\"\n\n    def test_basic_functionality(self, sample_config):\n        \"\"\"Test basic functionality with valid input.\"\"\"\n        instance = YourClass(sample_config)\n        result = instance.method()\n        assert result is not None\n\n    @pytest.mark.asyncio\n    async def test_async_method(self, sample_config):\n        \"\"\"Test async method execution.\"\"\"\n        instance = YourClass(sample_config)\n        result = await instance.async_method()\n        assert result[\"status\"] == \"success\"\n\n    def test_error_handling(self, sample_config):\n        \"\"\"Test error handling for invalid input.\"\"\"\n        instance = YourClass(sample_config)\n        with pytest.raises(ValueError):\n            instance.method(invalid_input=True)\n</code></pre>"},{"location":"quick_start/CONTRIBUTING/#troubleshooting","title":"Troubleshooting","text":""},{"location":"quick_start/CONTRIBUTING/#common-issues","title":"Common Issues","text":"<ol> <li>Import Errors: Ensure <code>PYTHONPATH</code> includes project root</li> <li>API Key Errors: Check that test environment uses mocked APIs</li> <li>Async Test Failures: Verify <code>pytest-asyncio</code> is installed</li> <li>Coverage Issues: Exclude test files from coverage reports</li> </ol>"},{"location":"quick_start/CONTRIBUTING/#getting-help","title":"Getting Help","text":"<ul> <li>Check existing issues and discussions</li> <li>Review test output and error messages</li> <li>Consult project documentation</li> <li>Ask questions in pull request comments</li> </ul>"},{"location":"quick_start/installation/","title":"Installation","text":"<p>First clone the repo:</p> <pre><code>git clone https://github.com/LINs-lab/MASArena\ncd MASArena/\n</code></pre>"},{"location":"quick_start/installation/#uv","title":"uv","text":"<p>We recommend using uv for dependency and virtual environment management. Refer to Installation for installation instructions.</p> <p>Run the installation with uv:</p> <pre><code>uv sync\n</code></pre>"},{"location":"quick_start/installation/#pip","title":"pip","text":"<p>Or you can use pip to install the dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"quick_start/usage/","title":"Usage","text":"<p>This guide explains how to run benchmarks and use the automated workflow optimizer with MASArena.</p>"},{"location":"quick_start/usage/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install dependencies:     If you haven't already, install the required packages. We recommend using <code>uv</code>.     <pre><code>uv sync\n</code></pre></p> </li> <li> <p>Configure Environment Variables:     Create a <code>.env</code> file in the project root and set your OpenAI API key and desired model.     <pre><code>OPENAI_API_KEY=your_openai_api_key\nMODEL_NAME=gpt-4o-mini\nOPENAI_API_BASE=https://api.openai.com/v1\n</code></pre></p> </li> </ol>"},{"location":"quick_start/usage/#running-benchmarks","title":"Running Benchmarks","text":"<p>You can run benchmarks using the convenience shell script <code>run_benchmark.sh</code> (recommended) or by directly calling <code>main.py</code>.</p>"},{"location":"quick_start/usage/#using-the-shell-script-run_benchmarksh","title":"Using the Shell Script (<code>run_benchmark.sh</code>)","text":"<p>The <code>run_benchmark.sh</code> script is the simplest way to run evaluations.</p> <p>Syntax: <pre><code># Usage: ./run_benchmark.sh [benchmark] [agent_system] [limit] [mcp_config] [concurrency] [optimizer]\n./run_benchmark.sh math supervisor_mas 10\n</code></pre></p> <p>Examples:</p> <pre><code># Run the 'math' benchmark on 10 problems with the 'supervisor_mas' agent system\n./run_benchmark.sh math supervisor_mas 10\n\n# Run the 'humaneval' benchmark asynchronously with a concurrency of 10\n# The \"\" is a placeholder for the mcp_config argument.\n./run_benchmark.sh humaneval single_agent 20 \"\" 10\n</code></pre>"},{"location":"quick_start/usage/#automated-workflow-optimization-aflow","title":"Automated Workflow Optimization (AFlow)","text":"<p>MASArena includes AFlow implementation, an automated optimizer for agent workflows. </p> <p>Example: To run AFlow to optimize an agent for the <code>humaneval</code> benchmark, provide <code>aflow</code> as the optimizer argument to the shell script:</p> <pre><code># The \"\" arguments are placeholders for mcp_config and concurrency.\n./run_benchmark.sh humaneval single_agent 10 \"\" \"\" aflow\n</code></pre> <p>You can also specify the training and test set sizes for the optimizer. Note that when using the <code>aflow</code> optimizer, the number of problems is determined by <code>train_size</code> and <code>test_size</code>, and the <code>limit</code> argument is ignored for data selection.</p> <p>Example with custom training and test sizes:</p> <pre><code># Run AFlow with a training set of 30 and a test set of 15.\n# The \"\" arguments are placeholders for mcp_config and concurrency.\n# The limit argument (10) is ignored in this case.\n./run_benchmark.sh humaneval single_agent 10 \"\" \"\" aflow 30 15\n</code></pre>"},{"location":"quick_start/usage/#command-line-arguments","title":"Command-Line Arguments","text":"<p>Here are the most common arguments for <code>main.py</code>.</p>"},{"location":"quick_start/usage/#main-arguments","title":"Main Arguments","text":"Argument Description Default <code>--benchmark</code> The name of the benchmark to run. <code>math</code> <code>--agent-system</code> The agent system to use for the benchmark. <code>single_agent</code> <code>--limit</code> The maximum number of problems to evaluate. <code>None</code> (all) <code>--data</code> Path to a custom benchmark data file (JSONL format). <code>data/{benchmark}_test.jsonl</code> <code>--results-dir</code> Directory to store detailed JSON results. <code>results/</code> <code>--verbose</code> Print progress information. <code>True</code> <code>--async-run</code> Run the benchmark asynchronously for faster evaluation. <code>False</code> <code>--concurrency</code> Set the concurrency level for asynchronous runs. <code>10</code> <code>--use-tools</code> Enable the agent to use integrated tools (e.g., code interpreter). <code>False</code> <code>--use-mcp-tools</code> Enable the agent to use tools via MCP. <code>False</code> <code>--mcp-config-file</code> Path to the MCP server configuration file. Required for MCP tools. <code>None</code> <code>--data-id</code> Data ID to use. <code>None</code>"},{"location":"quick_start/usage/#optimizer-arguments","title":"Optimizer Arguments","text":"<p>These arguments are used when running an optimizer like AFlow via <code>--run-optimizer</code>.</p> Argument Type Default Description <code>--run-optimizer</code> str <code>None</code> Specifies the optimizer to run. Use <code>aflow</code>. <code>--graph_path</code> str <code>mas_arena/configs/aflow</code> Path to the base AFlow graph configuration. <code>--optimized_path</code> str <code>example/aflow/humaneval/optimization</code> Path to save the optimized AFlow graph. <code>--validation_rounds</code> int 1 Number of validation rounds per optimization cycle. <code>--eval_rounds</code> int 1 Number of evaluation rounds per optimization cycle. <code>--max_rounds</code> int 3 Maximum number of optimization rounds. <code>--train_size</code> int 40 Size of the training set for evaluation. <code>--test_size</code> int 20 Size of the test set for evaluation."},{"location":"quick_start/usage/#example-output","title":"Example Output","text":"<p>After a run, a summary is printed to the console:</p> <pre><code>================================================================================\nBenchmark Summary\n================================================================================\nAgent system: swarm\nAccuracy: 70.00% (7/10)\nTotal duration: 335125ms\nResults saved to: results/math_swarm_20250616_203434.json\nSummary saved to: results/math_swarm_20250616_203434_summary.json\n\nRun visualization:\n$ python mas_arena/visualization/visualize_benchmark.py visualize \\\n  --summary results/math_swarm_20250616_203434_summary.json\n</code></pre>"},{"location":"quick_start/visualization/","title":"Visualizing Agent Interactions","text":"<p>You can generate an interactive HTML file to visualize agent message flows and other metadata from a completed benchmark run.</p> <pre><code>python mas_arena/visualization/visualize_benchmark.py visualize \\\n  --summary results/math_swarm_20250616_203434_summary.json\n</code></pre> <p>This is particularly useful for debugging and analyzing the behavior of multi-agent systems.</p> <p></p> <p></p>"},{"location":"tools/tool_integration/","title":"Tool Integration System","text":""},{"location":"tools/tool_integration/#overview","title":"Overview","text":"<p>The tool integration system allows agent systems to leverage external tools (like calculators, search engines, or specialized APIs) to enhance their problem-solving capabilities. Tool integration is implemented through three main components:</p>"},{"location":"tools/tool_integration/#components","title":"Components","text":"<ol> <li>ToolManager </li> <li>Loads and manages tool instances (MCP tools or mock tools).  </li> <li>Standardizes tools into a common dictionary format via <code>get_tools()</code>.  </li> <li> <p>Loads optional <code>tool_assignment</code> rules from the MCP configuration and exposes them via <code>get_tool_assignment_rules()</code>.</p> </li> <li> <p>ToolSelector </p> </li> <li>Provides a single public method <code>select_tools(task_description, num_agents=None, overlap=False, limit=5)</code>.  </li> <li>Internally uses <code>_select_for_task</code> for single-agent and <code>_partition_tools_for_multi_agent</code> for multi-agent.  </li> <li>Deprecated wrappers for the old <code>select_for_task</code> and <code>partition_tools_for_multi_agent</code> remain for backward compatibility.  </li> <li> <p>Override <code>select_tools</code> to implement custom selection algorithms.</p> </li> <li> <p>ToolIntegrationWrapper </p> </li> <li>Wraps an <code>AgentSystem</code> to inject tool integration without modifying agent code.  </li> <li>Initializes <code>ToolManager</code> and <code>ToolSelector</code> on the agent.  </li> <li>Patches multi-agent systems by intercepting <code>_create_agents</code>, applying <code>tool_assignment</code> rules if present, or falling back to <code>select_tools(...)</code>, then binds tools to each worker LLM.  </li> <li>Patches single-agent systems by intercepting <code>run_agent</code>, selecting tools before the run, and binding them to the agent's LLM.</li> </ol>"},{"location":"tools/tool_integration/#configuration","title":"Configuration","text":"<p>To enable MCP tool integration in the CLI or <code>run_benchmark.sh</code>, include the flags: <pre><code>--use-mcp-tools --mcp-config-file path/to/mcp_config.json\n</code></pre> The MCP config JSON can define both server endpoints and assignment rules:</p> <pre><code>{\n  \"math\": { \"command\": \"python\", \"args\": [\"math_server.py\"], \"transport\": \"stdio\" },\n  \"search\": { \"command\": \"python\", \"args\": [\"search_server.py\"], \"transport\": \"stdio\" },\n  \"tool_assignment\": {\n    \"MathAgent\": [\"add\", \"subtract\", \"solve_math\"],\n    \"SearchAgent\": [\"search\"],\n    \"Reasoner\": [\"infer\"]\n  }\n}\n</code></pre> <p><code>ToolManager.from_config_file</code> will split out the <code>tool_assignment</code> section into assignment rules and treat the remaining keys as MCP server definitions.</p>"},{"location":"tools/tool_integration/#how-they-work-together","title":"How They Work Together","text":"<pre><code>sequenceDiagram\n    participant TM as ToolManager\n    participant TIW as ToolIntegrationWrapper\n    participant TS as ToolSelector\n    participant Agents as AgentSystem\n\n    TM-&gt;&gt;TM: Load/create tools &amp; tool_assignment rules\n    TM-&gt;&gt;TIW: get_tools(), get_tool_assignment_rules()\n    TIW-&gt;&gt;TS: Initialize with full tool list\n    TS--&gt;&gt;TIW: Selector instance\n\n    loop For each problem\n        TIW-&gt;&gt;TIW: Check `tool_assignment_rules`\n        alt Assignment rules present\n            TIW-&gt;&gt;TIW: Assign tools per agent name\n        else No explicit rules\n            TIW-&gt;&gt;TS: select_tools(desc, num_agents)\n            TS--&gt;&gt;TIW: Return tool partitions or list\n        end\n        TIW-&gt;&gt;Agents: Bind selected tools to worker or LLM\n    end</code></pre>"},{"location":"tools/tool_integration/#developer-guide-implementing-a-tool-compatible-agentsystem","title":"Developer Guide: Implementing a Tool-Compatible AgentSystem","text":"<p>To make your <code>AgentSystem</code> (or its internal worker components) compatible with the <code>ToolIntegrationWrapper</code> and enable tool usage, follow these guidelines:</p> <p>Key Requirements:</p> <ol> <li> <p><code>llm</code> Attribute: Any component (the main agent system instance for single-agent systems, or individual worker objects in multi-agent systems) that is intended to have tools bound to its Language Model (LLM) must expose that LLM instance via an attribute named <code>llm</code>.     Example: <code>self.llm = ChatOpenAI(...)</code> or <code>worker_instance.llm = ChatOpenAI(...)</code></p> </li> <li> <p><code>name</code> Attribute: It's highly recommended that these components also have a <code>name</code> string attribute (e.g., <code>self.name = \"MyAgent\"</code> or <code>worker_instance.name = \"Researcher\"</code>). This name is used for:</p> <ul> <li>Logging by the <code>ToolIntegrationWrapper</code>.</li> <li>Targeted tool assignment if you use <code>tool_assignment</code> rules in your MCP configuration. If missing, a default name like <code>worker_0</code> will be used.</li> </ul> </li> <li> <p><code>_create_agents</code> Method (for Multi-Agent Systems):</p> <ul> <li>If you are building a multi-agent system where tools should be distributed to different worker agents, your <code>AgentSystem</code> subclass should implement a method:     <code>def _create_agents(self, problem_input: Optional[Any] = None, feedback: Optional[Any] = None) -&gt; Union[List[Any], Dict[str, Any]]:</code></li> <li>This method is called by <code>ToolIntegrationWrapper</code> to get the worker components.</li> <li>Return Value Flexibility:<ul> <li>Option 1 (Recommended for direct use in agent logic): Return a dictionary where keys are worker names (or any identifier) and values are the worker objects themselves. <code>ToolIntegrationWrapper</code> will process the values of this dictionary as the workers.     Example: <code>return {\"researcher\": self.research_agent_node, \"coder\": self.coder_agent_node}</code></li> <li>Option 2 (Simple List): Return a direct list of worker objects.     Example: <code>return [self.research_agent_node, self.coder_agent_node]</code></li> <li>Option 3 (Legacy): Return a dictionary <code>{\"workers\": [worker_object1, worker_object2, ...]}</code>.</li> </ul> </li> <li><code>ToolIntegrationWrapper</code> processes the identified worker objects in place (sets a <code>tools</code> attribute and rebinds their <code>llm</code> attribute if tools are assigned). The original structure returned by your <code>_create_agents</code> (list or dict) will be returned by the patched version, containing these modified worker objects.</li> </ul> </li> </ol> <p>Example: <code>SimpleSupervisor</code> (Multi-Agent System)</p> <p>Here's a simplified example demonstrating a supervisor agent system:</p> <pre><code>from typing import Dict, List, Any, Optional, Union\nfrom langchain_openai import ChatOpenAI\nfrom mas_arena.agents.base import AgentSystem # Assuming AgentSystem is in this path\n\n# A simple worker node\nclass SimpleAgentNode:\n    def __init__(self, name: str, model_name: str = \"gpt-4o-mini\"):\n        self.name = name\n        self.model_name = model_name\n        # Expose the LLM for ToolIntegrationWrapper\n        self.llm = ChatOpenAI(model=self.model_name)\n        # Tools will be assigned here by ToolIntegrationWrapper via setattr(self, \"tools\", ...)\n        # and self.llm will be rebound if tools are assigned.\n\n    def run(self, task: str) -&gt; str:\n        # In a real scenario, this would invoke self.llm with tools\n        # For this example, we just acknowledge the tools if present.\n        if hasattr(self, 'tools') and self.tools:\n            tool_names = [t.get('name', 'unknown_tool') for t in self.tools]\n            return f\"{self.name} would run task '{task}' with tools: {tool_names}\"\n        return f\"{self.name} would run task '{task}' (no tools assigned/bound)\"\n\n\nclass SimpleSupervisor(AgentSystem):\n    def __init__(self, name: str = \"SimpleSupervisor\", config: Dict[str, Any] = None):\n        super().__init__(name, config)\n        # The actual worker instances will be created and managed via _create_agents\n        # and the graph/logic in run_agent.\n        self.worker_nodes = None  # Populated by run_agent calling _init_workers\n        self.graph = None  # Placeholder for a more complex execution graph\n\n    def _create_agents(self, problem_input: Optional[Any] = None, feedback: Optional[Any] = None) -&gt; Dict[\n        str, SimpleAgentNode]:\n        \"\"\"Creates worker agents and returns them in a dictionary.\"\"\"\n        researcher_model = self.config.get(\"researcher_model\", \"gpt-4o-mini\")\n        coder_model = self.config.get(\"coder_model\", \"gpt-4o-mini\")\n\n        researcher = SimpleAgentNode(name=\"researcher\", model_name=researcher_model)\n        coder = SimpleAgentNode(name=\"coder\", model_name=coder_model)\n\n        # Return a dictionary; ToolIntegrationWrapper will process the values.\n        # The wrapper will modify these researcher and coder instances in place.\n        return {\n            \"researcher\": researcher,\n            \"coder\": coder\n        }\n\n    def _init_workers_if_needed(self, problem_input: Optional[Any] = None):\n        if self.worker_nodes is None:\n            # When wrapped, this calls the TIW-patched version of _create_agents.\n            # It receives the same dictionary structure, but the SimpleAgentNode\n            # instances within it will have been configured with tools by TIW.\n            self.worker_nodes = self._create_agents(problem_input=problem_input)\n\n    def run_agent(self, problem: Dict[str, Any], **kwargs) -&gt; Dict[str, Any]:\n        problem_content = problem.get(\"problem\", \"Analyze quantum entanglement.\")\n        self._init_workers_if_needed(problem_input=problem_content)\n\n        # Directly use the workers from the dictionary\n        researcher_node = self.worker_nodes[\"researcher\"]\n        coder_node = self.worker_nodes[\"coder\"]\n\n        # Example delegation (simplified)\n        research_result = researcher_node.run(f\"Research for: {problem_content}\")\n        final_result = coder_node.run(f\"Code based on: {research_result}\")\n\n        return {\"messages\": [(self.name, final_result)]}\n</code></pre> <p>How <code>ToolIntegrationWrapper</code> Interacts with <code>SimpleSupervisor</code>: 1.  When <code>SimpleSupervisor</code> is wrapped, <code>ToolIntegrationWrapper</code> patches its <code>_create_agents</code> method. 2.  During <code>simple_supervisor_instance.run_agent(...)</code>, the call to <code>self._init_workers_if_needed()</code> eventually calls the patched <code>_create_agents</code>. 3.  The original <code>_create_agents</code> logic runs, returning <code>{\"researcher\": researcher_node, \"coder\": coder_node}</code>. 4.  <code>ToolIntegrationWrapper</code> sees this dictionary. It iterates through its values (<code>researcher_node</code>, <code>coder_node</code>). 5.  For each node, it selects/assigns tools and binds them to <code>node.llm</code>. The <code>tools</code> attribute is also set on the node. 6.  The patched <code>_create_agents</code> returns the original dictionary structure, but the <code>researcher_node</code> and <code>coder_node</code> objects within it are now modified with tools and rebound LLMs. 7.  <code>_init_workers_if_needed</code> in <code>SimpleSupervisor</code> receives this dictionary and can directly use <code>self.worker_nodes[\"researcher\"]</code> etc., which are now tool-enabled.</p> <p>Single-Agent Systems:</p> <p>For agent systems that do not implement <code>_create_agents</code> (e.g., a single monolithic agent): - <code>ToolIntegrationWrapper</code> will attempt to patch the <code>run_agent</code> method. - It expects the agent system instance itself to have an <code>llm</code> attribute (e.g., <code>self.llm</code>). - Tools selected for the task will be bound to this top-level <code>self.llm</code>. - Ensure such agent systems have <code>self.name</code> and <code>self.llm</code> for proper integration.</p> <p>By adhering to these conventions, developers can more easily integrate their agent systems with the tool framework, leveraging its automated tool selection and LLM binding capabilities.</p>"}]}